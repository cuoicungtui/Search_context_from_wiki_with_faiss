{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T01:52:54.226636Z","iopub.execute_input":"2023-09-21T01:52:54.227246Z","iopub.status.idle":"2023-09-21T01:52:54.578937Z","shell.execute_reply.started":"2023-09-21T01:52:54.227217Z","shell.execute_reply":"2023-09-21T01:52:54.577979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:52:54.580862Z","iopub.execute_input":"2023-09-21T01:52:54.581324Z","iopub.status.idle":"2023-09-21T01:53:57.045149Z","shell.execute_reply.started":"2023-09-21T01:52:54.581289Z","shell.execute_reply":"2023-09-21T01:53:57.044026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:53:57.048276Z","iopub.execute_input":"2023-09-21T01:53:57.048623Z","iopub.status.idle":"2023-09-21T01:54:09.409026Z","shell.execute_reply.started":"2023-09-21T01:53:57.048591Z","shell.execute_reply":"2023-09-21T01:54:09.407864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n## Parameter to determine how many relevant sentences to include \\ Số lượng câu liên quan\nNUM_SENTENCES_INCLUDE = 25","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:09.411335Z","iopub.execute_input":"2023-09-21T01:54:09.411599Z","iopub.status.idle":"2023-09-21T01:54:09.424375Z","shell.execute_reply.started":"2023-09-21T01:54:09.411574Z","shell.execute_reply":"2023-09-21T01:54:09.423483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ctypes library to load the libc.so.6 shared library, which is the C library on Linux systems\n* _ = gc.collect()  : Forces an immediate garbage collection. This function collects and frees memory that is no longer needed.","metadata":{}},{"cell_type":"code","source":"trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntrn.drop(columns = 'id')\ntrn.head(4)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:09.425832Z","iopub.execute_input":"2023-09-21T01:54:09.426414Z","iopub.status.idle":"2023-09-21T01:54:09.462461Z","shell.execute_reply.started":"2023-09-21T01:54:09.426380Z","shell.execute_reply":"2023-09-21T01:54:09.461477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:09.463920Z","iopub.execute_input":"2023-09-21T01:54:09.464264Z","iopub.status.idle":"2023-09-21T01:54:10.811434Z","shell.execute_reply.started":"2023-09-21T01:54:09.464223Z","shell.execute_reply":"2023-09-21T01:54:10.810441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build index falatL2**","metadata":{}},{"cell_type":"markdown","source":"* Faiss (Facebook AI Similarity Search) để tìm kiếm hàng xóm gần nhất hiệu quả trong không gian chiều cao.\n* IndexFlat  là một lớp hoặc khái niệm thường được liên kết với các thư viện và khung để tìm kiếm hàng xóm gần đúng gần nhất. Nó thường được sử dụng trong bối cảnh học máy và các tác vụ truy xuất thông tin, nơi bạn cần tìm các điểm dữ liệu gần hoặc tương tự với một điểm truy vấn nhất định một cách hiệu quả.","metadata":{}},{"cell_type":"code","source":"# pip install faiss-cpu\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:10.812859Z","iopub.execute_input":"2023-09-21T01:54:10.813235Z","iopub.status.idle":"2023-09-21T01:54:10.822059Z","shell.execute_reply.started":"2023-09-21T01:54:10.813188Z","shell.execute_reply":"2023-09-21T01:54:10.819397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import faiss\n\n# # Sample data (you should replace this with your actual dataset)\n# data = np.random.rand(100, 64).astype(np.float32)\n\n# # Create an IndexFlatL2 index (L2 distance is commonly used for Euclidean distance)\n# dimension = data.shape[1]\n# index = faiss.IndexFlatL2(dimension)\n\n# # Add your data to the index\n# index.add(data)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:10.823563Z","iopub.execute_input":"2023-09-21T01:54:10.825004Z","iopub.status.idle":"2023-09-21T01:54:10.834041Z","shell.execute_reply.started":"2023-09-21T01:54:10.824952Z","shell.execute_reply":"2023-09-21T01:54:10.833013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load data wikipedia indexFlat**","metadata":{}},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:54:10.835588Z","iopub.execute_input":"2023-09-21T01:54:10.835987Z","iopub.status.idle":"2023-09-21T01:55:34.922096Z","shell.execute_reply.started":"2023-09-21T01:54:10.835935Z","shell.execute_reply":"2023-09-21T01:55:34.921021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(sentences = trn.prompt.values, batch_size= BATCH_SIZE, show_progress_bar = True,convert_to_tensor=True, device=DEVICE , normalize_embeddings =True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:55:34.927449Z","iopub.execute_input":"2023-09-21T01:55:34.927790Z","iopub.status.idle":"2023-09-21T01:55:41.830263Z","shell.execute_reply.started":"2023-09-21T01:55:34.927761Z","shell.execute_reply":"2023-09-21T01:55:41.829269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* .detach() : Phương pháp này được sử dụng để tạo một tensor mới chia sẻ cùng dữ liệu nhưng không được kết nối với biểu đồ tính toán. Nói cách khác, nó tách tensor khỏi mọi tính toán độ dốc. Điều này thường hữu ích khi bạn muốn chuyển đổi một tensor thành mảng NumPy vì bạn không cần độ dốc khi làm việc với kết quả cuối cùng.","metadata":{}},{"cell_type":"code","source":"## Get the top 3 pages that are likely to contain the topic of interest\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:55:41.831715Z","iopub.execute_input":"2023-09-21T01:55:41.832075Z","iopub.status.idle":"2023-09-21T01:56:02.835278Z","shell.execute_reply.started":"2023-09-21T01:55:41.832039Z","shell.execute_reply":"2023-09-21T01:56:02.834444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* libc.malloc_trim(0) Trong C, thường được sử dụng để trả lại bộ nhớ cho hệ thống, cung cấp bộ nhớ cho các tiến trình khác khi bạn đang sử dụng cấp phát bộ nhớ động với các hàm như . Đối số được truyền cho biết rằng bạn muốn giải phóng càng nhiều bộ nhớ trống càng tốt.","metadata":{}},{"cell_type":"code","source":"## Save memory - delete sentence_index since it is no longer necessary\ndel sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:02.839365Z","iopub.execute_input":"2023-09-21T01:56:02.841558Z","iopub.status.idle":"2023-09-21T01:56:03.686245Z","shell.execute_reply.started":"2023-09-21T01:56:02.841523Z","shell.execute_reply":"2023-09-21T01:56:03.685267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:03.687781Z","iopub.execute_input":"2023-09-21T01:56:03.689464Z","iopub.status.idle":"2023-09-21T01:56:07.964975Z","shell.execute_reply.started":"2023-09-21T01:56:03.689423Z","shell.execute_reply":"2023-09-21T01:56:07.963991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.loc[search_index[0]].copy()   lấy list row theo search_index to index df\nprint(search_index[0].shape)\nx = df.loc[search_index[0]].copy()\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:07.966072Z","iopub.execute_input":"2023-09-21T01:56:07.971962Z","iopub.status.idle":"2023-09-21T01:56:08.001786Z","shell.execute_reply.started":"2023-09-21T01:56:07.971920Z","shell.execute_reply":"2023-09-21T01:56:08.000618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the article and associated file location using the index\nwikipedia_file_data = []\n\nfor i ,(scr,idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx # list id\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.006407Z","iopub.execute_input":"2023-09-21T01:56:08.008643Z","iopub.status.idle":"2023-09-21T01:56:08.253172Z","shell.execute_reply.started":"2023-09-21T01:56:08.008606Z","shell.execute_reply":"2023-09-21T01:56:08.252348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nprint(wikipedia_file_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.257254Z","iopub.execute_input":"2023-09-21T01:56:08.259514Z","iopub.status.idle":"2023-09-21T01:56:08.291393Z","shell.execute_reply.started":"2023-09-21T01:56:08.259477Z","shell.execute_reply":"2023-09-21T01:56:08.290427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*sort datafame *\n* drop_duplicates()  xóa trùng lặp columns","metadata":{}},{"cell_type":"code","source":"wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.292672Z","iopub.execute_input":"2023-09-21T01:56:08.292998Z","iopub.status.idle":"2023-09-21T01:56:08.845641Z","shell.execute_reply.started":"2023-09-21T01:56:08.292973Z","shell.execute_reply":"2023-09-21T01:56:08.844708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.847835Z","iopub.execute_input":"2023-09-21T01:56:08.848886Z","iopub.status.idle":"2023-09-21T01:56:08.860511Z","shell.execute_reply.started":"2023-09-21T01:56:08.848830Z","shell.execute_reply":"2023-09-21T01:56:08.859160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.file.unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.861752Z","iopub.execute_input":"2023-09-21T01:56:08.865252Z","iopub.status.idle":"2023-09-21T01:56:08.872538Z","shell.execute_reply.started":"2023-09-21T01:56:08.865225Z","shell.execute_reply":"2023-09-21T01:56:08.871430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the full text data\n\"\"\"\n    for i in list title wikipedia parquet(a,b,c,..z)\n    _id - list id in title parquet (a/b/c/....)\n    read parquet file in list and read [id,text] and \n    get all text in _id ||  _df_temp = _df[_df['id'].isin(_id)].copy()\n    \n    Giải phóng dữ liệu sau mỗi lần đọc file parquet\n\"\"\"\nwiki_text_data = []\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data =  pd.concat(wiki_text_data).reset_index(drop=True)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T01:56:08.874407Z","iopub.execute_input":"2023-09-21T01:56:08.874791Z","iopub.status.idle":"2023-09-21T02:00:07.296606Z","shell.execute_reply.started":"2023-09-21T01:56:08.874759Z","shell.execute_reply":"2023-09-21T02:00:07.295581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**process_documents**\ninput \n1. sectionize_documents  Đọc lân lượt dữ liệu \n* input document_id and text :  Full document\n* sectionize_documents  function sử dụng trả về document_id , text and offset (Start,end) độ dài của text\n* sentencize  function là tách document thành list tupe chứa danh sách (stat , end) tách văn bản thành danh sách các câu lẻ và lưu số chỉ vị trí đầu và cuối của câu\n\n\nOUTPUT :  datafame chưa các hàng chứa từng câu đơn in text document và document_id and offset","metadata":{}},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \n    offset 0 len(documents)   : Độ dài của văn bản\n\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n    \n    \ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    \n    bf.text_to_sentences_and_offsets(document)\n    import blingfire as bf\n    text_to_sentences_and_offsets  tách các câu đơn từ doccumente -> return list tuper(start,end)  \n    \n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \n    Đầu ra tách nhỏ list tuper(start,end )  trả về datafame row từng tupe in list \n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:00:07.298259Z","iopub.execute_input":"2023-09-21T02:00:07.298880Z","iopub.status.idle":"2023-09-21T02:00:07.317063Z","shell.execute_reply.started":"2023-09-21T02:00:07.298827Z","shell.execute_reply":"2023-09-21T02:00:07.315995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test = sectionize_documents(documents =wiki_text_data.text.values, document_ids=wiki_text_data.id.values, disable_progress_bar =True)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:00:07.318902Z","iopub.execute_input":"2023-09-21T02:00:07.319419Z","iopub.status.idle":"2023-09-21T02:00:07.333132Z","shell.execute_reply.started":"2023-09-21T02:00:07.319380Z","shell.execute_reply":"2023-09-21T02:00:07.332105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parse documents into sentences\nprocessed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values,disable_progress_bar = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:01:00.462240Z","iopub.execute_input":"2023-09-21T02:01:00.462600Z","iopub.status.idle":"2023-09-21T02:01:07.290147Z","shell.execute_reply.started":"2023-09-21T02:01:00.462571Z","shell.execute_reply":"2023-09-21T02:01:07.289114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get embeddings of the wiki text data\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:01:39.856379Z","iopub.execute_input":"2023-09-21T02:01:39.856761Z","iopub.status.idle":"2023-09-21T02:02:14.707751Z","shell.execute_reply.started":"2023-09-21T02:01:39.856728Z","shell.execute_reply":"2023-09-21T02:02:14.706762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:02:27.671189Z","iopub.execute_input":"2023-09-21T02:02:27.671549Z","iopub.status.idle":"2023-09-21T02:02:27.689407Z","shell.execute_reply.started":"2023-09-21T02:02:27.671516Z","shell.execute_reply":"2023-09-21T02:02:27.688402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:02:31.039679Z","iopub.execute_input":"2023-09-21T02:02:31.040170Z","iopub.status.idle":"2023-09-21T02:02:31.059666Z","shell.execute_reply.started":"2023-09-21T02:02:31.040129Z","shell.execute_reply":"2023-09-21T02:02:31.058678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:02:34.732710Z","iopub.execute_input":"2023-09-21T02:02:34.733214Z","iopub.status.idle":"2023-09-21T02:02:35.199204Z","shell.execute_reply.started":"2023-09-21T02:02:34.733169Z","shell.execute_reply":"2023-09-21T02:02:35.198190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data.shape   # doc_id   text  offset","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:07:53.166618Z","iopub.execute_input":"2023-09-21T02:07:53.167021Z","iopub.status.idle":"2023-09-21T02:07:53.174563Z","shell.execute_reply.started":"2023-09-21T02:07:53.166985Z","shell.execute_reply":"2023-09-21T02:07:53.173666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_text_data.shape # id and text","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:08:01.551272Z","iopub.execute_input":"2023-09-21T02:08:01.552263Z","iopub.status.idle":"2023-09-21T02:08:01.559829Z","shell.execute_reply.started":"2023-09-21T02:08:01.552215Z","shell.execute_reply":"2023-09-21T02:08:01.558830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.head(3) # id prompt_id  and file","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:02:52.418289Z","iopub.execute_input":"2023-09-21T02:02:52.418730Z","iopub.status.idle":"2023-09-21T02:02:52.432593Z","shell.execute_reply.started":"2023-09-21T02:02:52.418690Z","shell.execute_reply":"2023-09-21T02:02:52.431435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data  # 1 prompt_id có 5 id document","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:08:44.682491Z","iopub.execute_input":"2023-09-21T02:08:44.682865Z","iopub.status.idle":"2023-09-21T02:08:44.698376Z","shell.execute_reply.started":"2023-09-21T02:08:44.682816Z","shell.execute_reply":"2023-09-21T02:08:44.697139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:30.104484Z","iopub.execute_input":"2023-09-21T02:15:30.105453Z","iopub.status.idle":"2023-09-21T02:15:30.125084Z","shell.execute_reply.started":"2023-09-21T02:15:30.105415Z","shell.execute_reply":"2023-09-21T02:15:30.124179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:28:03.565901Z","iopub.execute_input":"2023-09-21T02:28:03.566505Z","iopub.status.idle":"2023-09-21T02:28:03.583403Z","shell.execute_reply.started":"2023-09-21T02:28:03.566468Z","shell.execute_reply":"2023-09-21T02:28:03.582310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n## List containing just Context\ncontexts = []\n\n\n\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n    prompt_id = r.Index\n    \n    # list id_document  with prompt_id = index (len = 5) -> id  doccument\n    list_file_data_with_prompt_id = wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values\n    \n    # đánh dấu row có id document in list_prompt_id\n    list_text_in_document =  processed_wiki_text_data['document_id'].isin(list_file_data_with_prompt_id)\n    # read data index with bool = True\n    prompt_indices = processed_wiki_text_data[list_text_in_document].index.values\n    if prompt_indices.shape[0] > 0: \n        # create store Flat\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        # read index embedding and add faiss store flat ->  bộ dữ liệu ngữ cảnh\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n        context = \"\"\n        score, index = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(score[prompt_id], index[prompt_id]):\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n    contexts.append(context)   ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:33:13.323483Z","iopub.execute_input":"2023-09-21T02:33:13.323876Z","iopub.status.idle":"2023-09-21T02:33:16.790086Z","shell.execute_reply.started":"2023-09-21T02:33:13.323822Z","shell.execute_reply":"2023-09-21T02:33:16.789287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:34:31.077162Z","iopub.execute_input":"2023-09-21T02:34:31.077587Z","iopub.status.idle":"2023-09-21T02:34:31.083468Z","shell.execute_reply.started":"2023-09-21T02:34:31.077550Z","shell.execute_reply":"2023-09-21T02:34:31.082415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:35:11.514336Z","iopub.execute_input":"2023-09-21T02:35:11.514712Z","iopub.status.idle":"2023-09-21T02:35:11.594797Z","shell.execute_reply.started":"2023-09-21T02:35:11.514679Z","shell.execute_reply":"2023-09-21T02:35:11.593733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del trn\n# del contexts\n# del processed_wiki_text_data\n# del wikipedia_file_data\n# del all\n# _ = gc.collect()\n# libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:58:39.552500Z","iopub.execute_input":"2023-09-21T02:58:39.552884Z","iopub.status.idle":"2023-09-21T02:58:39.600822Z","shell.execute_reply.started":"2023-09-21T02:58:39.552830Z","shell.execute_reply":"2023-09-21T02:58:39.599512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for var_name in list(globals().keys()):\n#     if not var_name.startswith(\"__\"):  # To avoid deleting Python's built-in variables\n# #         del globals()[var_name]\n#         print(var_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T03:21:38.733445Z","iopub.execute_input":"2023-09-21T03:21:38.734101Z","iopub.status.idle":"2023-09-21T03:21:38.740164Z","shell.execute_reply.started":"2023-09-21T03:21:38.734066Z","shell.execute_reply":"2023-09-21T03:21:38.739015Z"},"trusted":true},"execution_count":null,"outputs":[]}]}